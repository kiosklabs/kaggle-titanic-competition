{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This contains the steps in solving the Titanic competition of Kaggle. \n",
    "\n",
    "This had been made after the completion of the DataQuest DS initial challenges/missions. So, it's advisable to go to DataQuest, since I might be missing additional explanation or discussion on the various steps made that may seem unclear to beginners.\n",
    "\n",
    "The Titanic competition involves predicting the survival rate through machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to instantiate requirement and prepare the data for training.\n",
    "\n",
    "Pandas is a very useful tool for processing datasets.\n",
    "We can easily format the csv file into usable data.\n",
    "As for sklearn(sci-kit learn), the machine learning and statistical functions it provides will be helpful later on in evaluating or validating the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived  Pclass  \\\n",
      "0              1         0       3   \n",
      "1              2         1       1   \n",
      "2              3         1       3   \n",
      "3              4         1       1   \n",
      "4              5         0       3   \n",
      "5              6         0       3   \n",
      "6              7         0       1   \n",
      "7              8         0       3   \n",
      "8              9         1       3   \n",
      "9             10         1       2   \n",
      "10            11         1       3   \n",
      "11            12         1       1   \n",
      "12            13         0       3   \n",
      "13            14         0       3   \n",
      "14            15         0       3   \n",
      "15            16         1       2   \n",
      "16            17         0       3   \n",
      "17            18         1       2   \n",
      "18            19         0       3   \n",
      "19            20         1       3   \n",
      "20            21         0       2   \n",
      "21            22         1       2   \n",
      "22            23         1       3   \n",
      "23            24         1       1   \n",
      "24            25         0       3   \n",
      "25            26         1       3   \n",
      "26            27         0       3   \n",
      "27            28         0       1   \n",
      "28            29         1       3   \n",
      "29            30         0       3   \n",
      "..           ...       ...     ...   \n",
      "861          862         0       2   \n",
      "862          863         1       1   \n",
      "863          864         0       3   \n",
      "864          865         0       2   \n",
      "865          866         1       2   \n",
      "866          867         1       2   \n",
      "867          868         0       1   \n",
      "868          869         0       3   \n",
      "869          870         1       3   \n",
      "870          871         0       3   \n",
      "871          872         1       1   \n",
      "872          873         0       1   \n",
      "873          874         0       3   \n",
      "874          875         1       2   \n",
      "875          876         1       3   \n",
      "876          877         0       3   \n",
      "877          878         0       3   \n",
      "878          879         0       3   \n",
      "879          880         1       1   \n",
      "880          881         1       2   \n",
      "881          882         0       3   \n",
      "882          883         0       3   \n",
      "883          884         0       2   \n",
      "884          885         0       3   \n",
      "885          886         0       3   \n",
      "886          887         0       2   \n",
      "887          888         1       1   \n",
      "888          889         0       3   \n",
      "889          890         1       1   \n",
      "890          891         0       3   \n",
      "\n",
      "                                                  Name     Sex   Age  SibSp  \\\n",
      "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                             Allen, Mr. William Henry    male  35.0      0   \n",
      "5                                     Moran, Mr. James    male   NaN      0   \n",
      "6                              McCarthy, Mr. Timothy J    male  54.0      0   \n",
      "7                       Palsson, Master. Gosta Leonard    male   2.0      3   \n",
      "8    Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n",
      "9                  Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n",
      "10                     Sandstrom, Miss. Marguerite Rut  female   4.0      1   \n",
      "11                            Bonnell, Miss. Elizabeth  female  58.0      0   \n",
      "12                      Saundercock, Mr. William Henry    male  20.0      0   \n",
      "13                         Andersson, Mr. Anders Johan    male  39.0      1   \n",
      "14                Vestrom, Miss. Hulda Amanda Adolfina  female  14.0      0   \n",
      "15                    Hewlett, Mrs. (Mary D Kingcome)   female  55.0      0   \n",
      "16                                Rice, Master. Eugene    male   2.0      4   \n",
      "17                        Williams, Mr. Charles Eugene    male   NaN      0   \n",
      "18   Vander Planke, Mrs. Julius (Emelia Maria Vande...  female  31.0      1   \n",
      "19                             Masselmani, Mrs. Fatima  female   NaN      0   \n",
      "20                                Fynney, Mr. Joseph J    male  35.0      0   \n",
      "21                               Beesley, Mr. Lawrence    male  34.0      0   \n",
      "22                         McGowan, Miss. Anna \"Annie\"  female  15.0      0   \n",
      "23                        Sloper, Mr. William Thompson    male  28.0      0   \n",
      "24                       Palsson, Miss. Torborg Danira  female   8.0      3   \n",
      "25   Asplund, Mrs. Carl Oscar (Selma Augusta Emilia...  female  38.0      1   \n",
      "26                             Emir, Mr. Farred Chehab    male   NaN      0   \n",
      "27                      Fortune, Mr. Charles Alexander    male  19.0      3   \n",
      "28                       O'Dwyer, Miss. Ellen \"Nellie\"  female   NaN      0   \n",
      "29                                 Todoroff, Mr. Lalio    male   NaN      0   \n",
      "..                                                 ...     ...   ...    ...   \n",
      "861                        Giles, Mr. Frederick Edward    male  21.0      1   \n",
      "862  Swift, Mrs. Frederick Joel (Margaret Welles Ba...  female  48.0      0   \n",
      "863                  Sage, Miss. Dorothy Edith \"Dolly\"  female   NaN      8   \n",
      "864                             Gill, Mr. John William    male  24.0      0   \n",
      "865                           Bystrom, Mrs. (Karolina)  female  42.0      0   \n",
      "866                       Duran y More, Miss. Asuncion  female  27.0      1   \n",
      "867               Roebling, Mr. Washington Augustus II    male  31.0      0   \n",
      "868                        van Melkebeke, Mr. Philemon    male   NaN      0   \n",
      "869                    Johnson, Master. Harold Theodor    male   4.0      1   \n",
      "870                                  Balkic, Mr. Cerin    male  26.0      0   \n",
      "871   Beckwith, Mrs. Richard Leonard (Sallie Monypeny)  female  47.0      1   \n",
      "872                           Carlsson, Mr. Frans Olof    male  33.0      0   \n",
      "873                        Vander Cruyssen, Mr. Victor    male  47.0      0   \n",
      "874              Abelson, Mrs. Samuel (Hannah Wizosky)  female  28.0      1   \n",
      "875                   Najib, Miss. Adele Kiamie \"Jane\"  female  15.0      0   \n",
      "876                      Gustafsson, Mr. Alfred Ossian    male  20.0      0   \n",
      "877                               Petroff, Mr. Nedelio    male  19.0      0   \n",
      "878                                 Laleff, Mr. Kristo    male   NaN      0   \n",
      "879      Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)  female  56.0      0   \n",
      "880       Shelley, Mrs. William (Imanita Parrish Hall)  female  25.0      0   \n",
      "881                                 Markun, Mr. Johann    male  33.0      0   \n",
      "882                       Dahlberg, Miss. Gerda Ulrika  female  22.0      0   \n",
      "883                      Banfield, Mr. Frederick James    male  28.0      0   \n",
      "884                             Sutehall, Mr. Henry Jr    male  25.0      0   \n",
      "885               Rice, Mrs. William (Margaret Norton)  female  39.0      0   \n",
      "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
      "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
      "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
      "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
      "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
      "\n",
      "     Parch            Ticket      Fare        Cabin Embarked  \n",
      "0        0         A/5 21171    7.2500          NaN        S  \n",
      "1        0          PC 17599   71.2833          C85        C  \n",
      "2        0  STON/O2. 3101282    7.9250          NaN        S  \n",
      "3        0            113803   53.1000         C123        S  \n",
      "4        0            373450    8.0500          NaN        S  \n",
      "5        0            330877    8.4583          NaN        Q  \n",
      "6        0             17463   51.8625          E46        S  \n",
      "7        1            349909   21.0750          NaN        S  \n",
      "8        2            347742   11.1333          NaN        S  \n",
      "9        0            237736   30.0708          NaN        C  \n",
      "10       1           PP 9549   16.7000           G6        S  \n",
      "11       0            113783   26.5500         C103        S  \n",
      "12       0         A/5. 2151    8.0500          NaN        S  \n",
      "13       5            347082   31.2750          NaN        S  \n",
      "14       0            350406    7.8542          NaN        S  \n",
      "15       0            248706   16.0000          NaN        S  \n",
      "16       1            382652   29.1250          NaN        Q  \n",
      "17       0            244373   13.0000          NaN        S  \n",
      "18       0            345763   18.0000          NaN        S  \n",
      "19       0              2649    7.2250          NaN        C  \n",
      "20       0            239865   26.0000          NaN        S  \n",
      "21       0            248698   13.0000          D56        S  \n",
      "22       0            330923    8.0292          NaN        Q  \n",
      "23       0            113788   35.5000           A6        S  \n",
      "24       1            349909   21.0750          NaN        S  \n",
      "25       5            347077   31.3875          NaN        S  \n",
      "26       0              2631    7.2250          NaN        C  \n",
      "27       2             19950  263.0000  C23 C25 C27        S  \n",
      "28       0            330959    7.8792          NaN        Q  \n",
      "29       0            349216    7.8958          NaN        S  \n",
      "..     ...               ...       ...          ...      ...  \n",
      "861      0             28134   11.5000          NaN        S  \n",
      "862      0             17466   25.9292          D17        S  \n",
      "863      2          CA. 2343   69.5500          NaN        S  \n",
      "864      0            233866   13.0000          NaN        S  \n",
      "865      0            236852   13.0000          NaN        S  \n",
      "866      0     SC/PARIS 2149   13.8583          NaN        C  \n",
      "867      0          PC 17590   50.4958          A24        S  \n",
      "868      0            345777    9.5000          NaN        S  \n",
      "869      1            347742   11.1333          NaN        S  \n",
      "870      0            349248    7.8958          NaN        S  \n",
      "871      1             11751   52.5542          D35        S  \n",
      "872      0               695    5.0000  B51 B53 B55        S  \n",
      "873      0            345765    9.0000          NaN        S  \n",
      "874      0         P/PP 3381   24.0000          NaN        C  \n",
      "875      0              2667    7.2250          NaN        C  \n",
      "876      0              7534    9.8458          NaN        S  \n",
      "877      0            349212    7.8958          NaN        S  \n",
      "878      0            349217    7.8958          NaN        S  \n",
      "879      1             11767   83.1583          C50        C  \n",
      "880      1            230433   26.0000          NaN        S  \n",
      "881      0            349257    7.8958          NaN        S  \n",
      "882      0              7552   10.5167          NaN        S  \n",
      "883      0  C.A./SOTON 34068   10.5000          NaN        S  \n",
      "884      0   SOTON/OQ 392076    7.0500          NaN        S  \n",
      "885      5            382652   29.1250          NaN        Q  \n",
      "886      0            211536   13.0000          NaN        S  \n",
      "887      0            112053   30.0000          B42        S  \n",
      "888      2        W./C. 6607   23.4500          NaN        S  \n",
      "889      0            111369   30.0000         C148        C  \n",
      "890      0            370376    7.7500          NaN        Q  \n",
      "\n",
      "[891 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv as csv\n",
    "\n",
    "training_df = pd.read_csv('data/train.csv', header=0)\n",
    "training_df.describe()\n",
    "print training_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen in the data, that more than few values are missing in some columns such as Age or Embarked.\n",
    "Since we are try to determine if these predictors have a relationship to survivability, it is vital to clean up and prepare the training data to minimize 'noise' so as not to skew the accuracy of predictions. This is what we call 'overfitting' when inaccurate data is covered by machine learning, which generates inaccurate results that might lead to wrong conclusions.\n",
    "\n",
    "We'll do this by using the median (not to be confused with mean) of the set, and provide a default value to missing data for the Embarked column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_df[\"Age\"] = training_df[\"Age\"].fillna(training_df[\"Age\"].median())\n",
    "training_df[\"Embarked\"] = training_df[\"Embarked\"].fillna(\"S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also requires transforming some data to ordinal symbols for our classifier later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_df.loc[training_df[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "training_df.loc[training_df[\"Sex\"] == \"female\", \"Sex\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_df.loc[training_df[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "training_df.loc[training_df[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "training_df.loc[training_df[\"Embarked\"] == \"Q\", \"Embarked\"] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've prepared the data, we'll use linear regression first to see if there are direct relationships to survivability. We can also test our initial assumptions when looking at the data before we can draw a conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.783389450056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rogue/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:27: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import cross_validation\n",
    "\n",
    "predictors =  [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "alg = LinearRegression()\n",
    "\n",
    "kf = KFold(training_df.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    train_predictors = (training_df[predictors].iloc[train,:])\n",
    "    train_target = training_df[\"Survived\"].iloc[train]\n",
    "    alg.fit(train_predictors, train_target)\n",
    "    \n",
    "    test_predictions = alg.predict(training_df[predictors].iloc[test,:])\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "#since predictions are in 3 (from n-folds) separate arrays (combine them)\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "#map predictions to outcomes (>.5 survives(1), <= .5 (dies))\n",
    "predictions[predictions > 0.5] = 1\n",
    "predictions[predictions <= 0.5] = 0\n",
    "\n",
    "#then we check and compare the accuracy\n",
    "accuracy = sum(predictions[predictions == training_df[\"Survived\"]]) / len(predictions)\n",
    "\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, we only have 78 percent accuracy when using linear regression. This can easily be explained that in predictors such as Age, while there are indeed many who didn't survived accordingly, there are also cases wherein some survived. \n",
    "\n",
    "We can also use cross_validation to validate our predictors much more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.787878787879\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "titanic_test = pandas.read_csv(\"data/train.csv\")\n",
    "\n",
    "titanic_test['Age'] = titanic_test['Age'].fillna(titanic_test['Age'].median())\n",
    "titanic_test.loc[titanic_test['Sex'] == 'male', 'Sex'] = 0\n",
    "titanic_test.loc[titanic_test['Sex'] == 'female', 'Sex'] = 1\n",
    "\n",
    "titanic_test['Embarked'] = titanic_test['Embarked'].fillna('S')\n",
    "titanic_test.loc[titanic_test['Embarked']=='S', 'Embarked'] = 0\n",
    "titanic_test.loc[titanic_test['Embarked']=='C', 'Embarked'] = 1\n",
    "titanic_test.loc[titanic_test['Embarked']=='Q', 'Embarked'] = 2\n",
    "\n",
    "titanic_test['Fare'] = titanic_test['Fare'].fillna(titanic_test['Fare'].median())\n",
    "\n",
    "alg = LogisticRegression(random_state=1)\n",
    "scores = cross_validation.cross_val_score(alg, titanic_test[predictors], titanic_test[\"Survived\"], cv=3)\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can improve this by using a Random Forest classifier, which is a better algorithm for the non-linear tendencies/relationships in our dataset. Simply put, the random forest algorithm involves creating decision trees that have slightly randomized input data and splitpoint, and evaluating the overall average prediction of the trees.\n",
    "\n",
    "We can use the RandomForestClassifier implementation in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.801346801347\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "titanic = pandas.read_csv(\"data/train.csv\")\n",
    "titanic['Age'] = titanic['Age'].fillna(titanic['Age'].median())\n",
    "titanic.loc[titanic['Sex'] == 'male', 'Sex'] = 0\n",
    "titanic.loc[titanic['Sex'] == 'female', 'Sex'] = 1\n",
    "\n",
    "titanic['Embarked'] = titanic['Embarked'].fillna('S')\n",
    "titanic.loc[titanic['Embarked']=='S', 'Embarked'] = 0\n",
    "titanic.loc[titanic['Embarked']=='C', 'Embarked'] = 1\n",
    "titanic.loc[titanic['Embarked']=='Q', 'Embarked'] = 2\n",
    "\n",
    "titanic['Fare'] = titanic['Fare'].fillna(titanic['Fare'].median())\n",
    "# Initialize our algorithm with the default paramters\n",
    "# n_estimators is the number of trees we want to make\n",
    "# min_samples_split is the minimum number of rows we need to make a split\n",
    "# min_samples_leaf is the minimum number of samples we can have at the place where a tree branch ends (the bottom points of the tree)\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, min_samples_leaf=1)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic['Survived'], cv=3)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Random Forests, we've increased the accuracy of our predictions. However, we can tweak the parameters such as no. of trees (n_estimators), the no. of splits in our decision trees (min_samples_split) and sample leafs (min_samples_leaf) to minimize overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.820426487093\n"
     ]
    }
   ],
   "source": [
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=4, min_samples_leaf=2)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic['Survived'], cv=3)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing the sample size to 150, splits to 4 and leafs to 2, we've increased the accuracy score to around 2 percent.\n",
    "However, we still have other columns in the data that we can test to increase the overall accuracy of our algorithm.\n",
    "\n",
    "One of those is testing whether the name could be a factor (as unlikely as it sounds). This is done through feature engineering, namely, by creating a new feature, by identifying key information in the data which can help us increase the accuracy of our predictions.\n",
    "\n",
    "In this case, our data includes the titles of the passengers. We can thus, generate a new feature by extracting their titles and including it in our predictors.\n",
    "\n",
    "Let's add a few features that could help us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add Features FamilySize, because more people can support each other to survive\n",
    "# Create total count (SiblingSpouse + ParentChild)\n",
    "titanic[\"FamilySize\"] = titanic[\"SibSp\"] + titanic[\"Parch\"]\n",
    "\n",
    "# Add Name Length because the longer the name, the more important sounding the invidual might be\n",
    "titanic[\"NameLength\"] = titanic[\"Name\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data also includes the titles of the passengers. We can thus, generate a new feature by extracting their titles and including it in our predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr          517\n",
      "Miss        182\n",
      "Mrs         125\n",
      "Master       40\n",
      "Dr            7\n",
      "Rev           6\n",
      "Col           2\n",
      "Major         2\n",
      "Mlle          2\n",
      "Countess      1\n",
      "Ms            1\n",
      "Lady          1\n",
      "Jonkheer      1\n",
      "Don           1\n",
      "Mme           1\n",
      "Capt          1\n",
      "Sir           1\n",
      "Name: Name, dtype: int64\n",
      "1     517\n",
      "2     183\n",
      "3     125\n",
      "4      40\n",
      "5       7\n",
      "6       6\n",
      "7       5\n",
      "10      3\n",
      "8       3\n",
      "9       2\n",
      "Name: Name, dtype: int64\n",
      "     PassengerId  Survived  Pclass  \\\n",
      "0              1         0       3   \n",
      "1              2         1       1   \n",
      "2              3         1       3   \n",
      "3              4         1       1   \n",
      "4              5         0       3   \n",
      "5              6         0       3   \n",
      "6              7         0       1   \n",
      "7              8         0       3   \n",
      "8              9         1       3   \n",
      "9             10         1       2   \n",
      "10            11         1       3   \n",
      "11            12         1       1   \n",
      "12            13         0       3   \n",
      "13            14         0       3   \n",
      "14            15         0       3   \n",
      "15            16         1       2   \n",
      "16            17         0       3   \n",
      "17            18         1       2   \n",
      "18            19         0       3   \n",
      "19            20         1       3   \n",
      "20            21         0       2   \n",
      "21            22         1       2   \n",
      "22            23         1       3   \n",
      "23            24         1       1   \n",
      "24            25         0       3   \n",
      "25            26         1       3   \n",
      "26            27         0       3   \n",
      "27            28         0       1   \n",
      "28            29         1       3   \n",
      "29            30         0       3   \n",
      "..           ...       ...     ...   \n",
      "861          862         0       2   \n",
      "862          863         1       1   \n",
      "863          864         0       3   \n",
      "864          865         0       2   \n",
      "865          866         1       2   \n",
      "866          867         1       2   \n",
      "867          868         0       1   \n",
      "868          869         0       3   \n",
      "869          870         1       3   \n",
      "870          871         0       3   \n",
      "871          872         1       1   \n",
      "872          873         0       1   \n",
      "873          874         0       3   \n",
      "874          875         1       2   \n",
      "875          876         1       3   \n",
      "876          877         0       3   \n",
      "877          878         0       3   \n",
      "878          879         0       3   \n",
      "879          880         1       1   \n",
      "880          881         1       2   \n",
      "881          882         0       3   \n",
      "882          883         0       3   \n",
      "883          884         0       2   \n",
      "884          885         0       3   \n",
      "885          886         0       3   \n",
      "886          887         0       2   \n",
      "887          888         1       1   \n",
      "888          889         0       3   \n",
      "889          890         1       1   \n",
      "890          891         0       3   \n",
      "\n",
      "                                                  Name Sex   Age  SibSp  \\\n",
      "0                              Braund, Mr. Owen Harris   0  22.0      1   \n",
      "1    Cumings, Mrs. John Bradley (Florence Briggs Th...   1  38.0      1   \n",
      "2                               Heikkinen, Miss. Laina   1  26.0      0   \n",
      "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)   1  35.0      1   \n",
      "4                             Allen, Mr. William Henry   0  35.0      0   \n",
      "5                                     Moran, Mr. James   0  28.0      0   \n",
      "6                              McCarthy, Mr. Timothy J   0  54.0      0   \n",
      "7                       Palsson, Master. Gosta Leonard   0   2.0      3   \n",
      "8    Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)   1  27.0      0   \n",
      "9                  Nasser, Mrs. Nicholas (Adele Achem)   1  14.0      1   \n",
      "10                     Sandstrom, Miss. Marguerite Rut   1   4.0      1   \n",
      "11                            Bonnell, Miss. Elizabeth   1  58.0      0   \n",
      "12                      Saundercock, Mr. William Henry   0  20.0      0   \n",
      "13                         Andersson, Mr. Anders Johan   0  39.0      1   \n",
      "14                Vestrom, Miss. Hulda Amanda Adolfina   1  14.0      0   \n",
      "15                    Hewlett, Mrs. (Mary D Kingcome)    1  55.0      0   \n",
      "16                                Rice, Master. Eugene   0   2.0      4   \n",
      "17                        Williams, Mr. Charles Eugene   0  28.0      0   \n",
      "18   Vander Planke, Mrs. Julius (Emelia Maria Vande...   1  31.0      1   \n",
      "19                             Masselmani, Mrs. Fatima   1  28.0      0   \n",
      "20                                Fynney, Mr. Joseph J   0  35.0      0   \n",
      "21                               Beesley, Mr. Lawrence   0  34.0      0   \n",
      "22                         McGowan, Miss. Anna \"Annie\"   1  15.0      0   \n",
      "23                        Sloper, Mr. William Thompson   0  28.0      0   \n",
      "24                       Palsson, Miss. Torborg Danira   1   8.0      3   \n",
      "25   Asplund, Mrs. Carl Oscar (Selma Augusta Emilia...   1  38.0      1   \n",
      "26                             Emir, Mr. Farred Chehab   0  28.0      0   \n",
      "27                      Fortune, Mr. Charles Alexander   0  19.0      3   \n",
      "28                       O'Dwyer, Miss. Ellen \"Nellie\"   1  28.0      0   \n",
      "29                                 Todoroff, Mr. Lalio   0  28.0      0   \n",
      "..                                                 ...  ..   ...    ...   \n",
      "861                        Giles, Mr. Frederick Edward   0  21.0      1   \n",
      "862  Swift, Mrs. Frederick Joel (Margaret Welles Ba...   1  48.0      0   \n",
      "863                  Sage, Miss. Dorothy Edith \"Dolly\"   1  28.0      8   \n",
      "864                             Gill, Mr. John William   0  24.0      0   \n",
      "865                           Bystrom, Mrs. (Karolina)   1  42.0      0   \n",
      "866                       Duran y More, Miss. Asuncion   1  27.0      1   \n",
      "867               Roebling, Mr. Washington Augustus II   0  31.0      0   \n",
      "868                        van Melkebeke, Mr. Philemon   0  28.0      0   \n",
      "869                    Johnson, Master. Harold Theodor   0   4.0      1   \n",
      "870                                  Balkic, Mr. Cerin   0  26.0      0   \n",
      "871   Beckwith, Mrs. Richard Leonard (Sallie Monypeny)   1  47.0      1   \n",
      "872                           Carlsson, Mr. Frans Olof   0  33.0      0   \n",
      "873                        Vander Cruyssen, Mr. Victor   0  47.0      0   \n",
      "874              Abelson, Mrs. Samuel (Hannah Wizosky)   1  28.0      1   \n",
      "875                   Najib, Miss. Adele Kiamie \"Jane\"   1  15.0      0   \n",
      "876                      Gustafsson, Mr. Alfred Ossian   0  20.0      0   \n",
      "877                               Petroff, Mr. Nedelio   0  19.0      0   \n",
      "878                                 Laleff, Mr. Kristo   0  28.0      0   \n",
      "879      Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)   1  56.0      0   \n",
      "880       Shelley, Mrs. William (Imanita Parrish Hall)   1  25.0      0   \n",
      "881                                 Markun, Mr. Johann   0  33.0      0   \n",
      "882                       Dahlberg, Miss. Gerda Ulrika   1  22.0      0   \n",
      "883                      Banfield, Mr. Frederick James   0  28.0      0   \n",
      "884                             Sutehall, Mr. Henry Jr   0  25.0      0   \n",
      "885               Rice, Mrs. William (Margaret Norton)   1  39.0      0   \n",
      "886                              Montvila, Rev. Juozas   0  27.0      0   \n",
      "887                       Graham, Miss. Margaret Edith   1  19.0      0   \n",
      "888           Johnston, Miss. Catherine Helen \"Carrie\"   1  28.0      1   \n",
      "889                              Behr, Mr. Karl Howell   0  26.0      0   \n",
      "890                                Dooley, Mr. Patrick   0  32.0      0   \n",
      "\n",
      "     Parch            Ticket      Fare        Cabin Embarked  FamilySize  \\\n",
      "0        0         A/5 21171    7.2500          NaN        0           1   \n",
      "1        0          PC 17599   71.2833          C85        1           1   \n",
      "2        0  STON/O2. 3101282    7.9250          NaN        0           0   \n",
      "3        0            113803   53.1000         C123        0           1   \n",
      "4        0            373450    8.0500          NaN        0           0   \n",
      "5        0            330877    8.4583          NaN        2           0   \n",
      "6        0             17463   51.8625          E46        0           0   \n",
      "7        1            349909   21.0750          NaN        0           4   \n",
      "8        2            347742   11.1333          NaN        0           2   \n",
      "9        0            237736   30.0708          NaN        1           1   \n",
      "10       1           PP 9549   16.7000           G6        0           2   \n",
      "11       0            113783   26.5500         C103        0           0   \n",
      "12       0         A/5. 2151    8.0500          NaN        0           0   \n",
      "13       5            347082   31.2750          NaN        0           6   \n",
      "14       0            350406    7.8542          NaN        0           0   \n",
      "15       0            248706   16.0000          NaN        0           0   \n",
      "16       1            382652   29.1250          NaN        2           5   \n",
      "17       0            244373   13.0000          NaN        0           0   \n",
      "18       0            345763   18.0000          NaN        0           1   \n",
      "19       0              2649    7.2250          NaN        1           0   \n",
      "20       0            239865   26.0000          NaN        0           0   \n",
      "21       0            248698   13.0000          D56        0           0   \n",
      "22       0            330923    8.0292          NaN        2           0   \n",
      "23       0            113788   35.5000           A6        0           0   \n",
      "24       1            349909   21.0750          NaN        0           4   \n",
      "25       5            347077   31.3875          NaN        0           6   \n",
      "26       0              2631    7.2250          NaN        1           0   \n",
      "27       2             19950  263.0000  C23 C25 C27        0           5   \n",
      "28       0            330959    7.8792          NaN        2           0   \n",
      "29       0            349216    7.8958          NaN        0           0   \n",
      "..     ...               ...       ...          ...      ...         ...   \n",
      "861      0             28134   11.5000          NaN        0           1   \n",
      "862      0             17466   25.9292          D17        0           0   \n",
      "863      2          CA. 2343   69.5500          NaN        0          10   \n",
      "864      0            233866   13.0000          NaN        0           0   \n",
      "865      0            236852   13.0000          NaN        0           0   \n",
      "866      0     SC/PARIS 2149   13.8583          NaN        1           1   \n",
      "867      0          PC 17590   50.4958          A24        0           0   \n",
      "868      0            345777    9.5000          NaN        0           0   \n",
      "869      1            347742   11.1333          NaN        0           2   \n",
      "870      0            349248    7.8958          NaN        0           0   \n",
      "871      1             11751   52.5542          D35        0           2   \n",
      "872      0               695    5.0000  B51 B53 B55        0           0   \n",
      "873      0            345765    9.0000          NaN        0           0   \n",
      "874      0         P/PP 3381   24.0000          NaN        1           1   \n",
      "875      0              2667    7.2250          NaN        1           0   \n",
      "876      0              7534    9.8458          NaN        0           0   \n",
      "877      0            349212    7.8958          NaN        0           0   \n",
      "878      0            349217    7.8958          NaN        0           0   \n",
      "879      1             11767   83.1583          C50        1           1   \n",
      "880      1            230433   26.0000          NaN        0           1   \n",
      "881      0            349257    7.8958          NaN        0           0   \n",
      "882      0              7552   10.5167          NaN        0           0   \n",
      "883      0  C.A./SOTON 34068   10.5000          NaN        0           0   \n",
      "884      0   SOTON/OQ 392076    7.0500          NaN        0           0   \n",
      "885      5            382652   29.1250          NaN        2           5   \n",
      "886      0            211536   13.0000          NaN        0           0   \n",
      "887      0            112053   30.0000          B42        0           0   \n",
      "888      2        W./C. 6607   23.4500          NaN        0           3   \n",
      "889      0            111369   30.0000         C148        1           0   \n",
      "890      0            370376    7.7500          NaN        2           0   \n",
      "\n",
      "     NameLength Title  \n",
      "0            23     1  \n",
      "1            51     3  \n",
      "2            22     2  \n",
      "3            44     3  \n",
      "4            24     1  \n",
      "5            16     1  \n",
      "6            23     1  \n",
      "7            30     4  \n",
      "8            49     3  \n",
      "9            35     3  \n",
      "10           31     2  \n",
      "11           24     2  \n",
      "12           30     1  \n",
      "13           27     1  \n",
      "14           36     2  \n",
      "15           32     3  \n",
      "16           20     4  \n",
      "17           28     1  \n",
      "18           55     3  \n",
      "19           23     3  \n",
      "20           20     1  \n",
      "21           21     1  \n",
      "22           27     2  \n",
      "23           28     1  \n",
      "24           29     2  \n",
      "25           57     3  \n",
      "26           23     1  \n",
      "27           30     1  \n",
      "28           29     2  \n",
      "29           19     1  \n",
      "..          ...   ...  \n",
      "861          27     1  \n",
      "862          51     3  \n",
      "863          33     2  \n",
      "864          22     1  \n",
      "865          24     3  \n",
      "866          28     2  \n",
      "867          36     1  \n",
      "868          27     1  \n",
      "869          31     4  \n",
      "870          17     1  \n",
      "871          48     3  \n",
      "872          24     1  \n",
      "873          27     1  \n",
      "874          37     3  \n",
      "875          32     2  \n",
      "876          29     1  \n",
      "877          20     1  \n",
      "878          18     1  \n",
      "879          45     3  \n",
      "880          44     3  \n",
      "881          18     1  \n",
      "882          28     2  \n",
      "883          29     1  \n",
      "884          22     1  \n",
      "885          36     3  \n",
      "886          21     6  \n",
      "887          28     2  \n",
      "888          40     2  \n",
      "889          21     1  \n",
      "890          19     1  \n",
      "\n",
      "[891 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# A function to get the title from a name.\n",
    "def get_title(name):\n",
    "    # Use a regular expression to search for a title.  Titles always consist of capital and lowercase letters, and end with a period.\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "# Get all the titles and print how often each one occurs.\n",
    "titles = titanic[\"Name\"].apply(get_title)\n",
    "print(pandas.value_counts(titles))\n",
    "\n",
    "# Map each title to an integer.  Some titles are very rare, and are compressed into the same codes as other titles.\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "\n",
    "# Verify that we converted everything.\n",
    "print(pandas.value_counts(titles))\n",
    "\n",
    "# Add in the title column.\n",
    "titanic[\"Title\"] = titles\n",
    "print titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've extracted the title from the names, we can group the families together. Of course, the more members of the family there are, the more likely it is that they'll help each other to survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1      800\n",
      " 14       8\n",
      " 149      7\n",
      " 63       6\n",
      " 50       6\n",
      " 59       6\n",
      " 17       5\n",
      " 384      4\n",
      " 27       4\n",
      " 25       4\n",
      " 162      4\n",
      " 8        4\n",
      " 84       4\n",
      " 340      4\n",
      " 43       3\n",
      " 269      3\n",
      " 58       3\n",
      " 633      2\n",
      " 167      2\n",
      " 280      2\n",
      " 510      2\n",
      " 90       2\n",
      " 83       1\n",
      " 625      1\n",
      " 376      1\n",
      " 449      1\n",
      " 498      1\n",
      " 588      1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "# A dictionary mapping family name to id\n",
    "family_id_mapping = {}\n",
    "\n",
    "# A function to get the id given a row\n",
    "def get_family_id(row):\n",
    "    # Find the last name by splitting on a comma\n",
    "    last_name = row[\"Name\"].split(\",\")[0]\n",
    "    # Create the family id\n",
    "    family_id = \"{0}{1}\".format(last_name, row[\"FamilySize\"])\n",
    "    # Look up the id in the mapping\n",
    "    if family_id not in family_id_mapping:\n",
    "        if len(family_id_mapping) == 0:\n",
    "            current_id = 1\n",
    "        else:\n",
    "            # Get the maximum id from the mapping and add one to it if we don't have an id\n",
    "            current_id = (max(family_id_mapping.items(), key=operator.itemgetter(1))[1] + 1)\n",
    "        family_id_mapping[family_id] = current_id\n",
    "    return family_id_mapping[family_id]\n",
    "\n",
    "# Get the family ids with the apply method\n",
    "family_ids = titanic.apply(get_family_id, axis=1)\n",
    "\n",
    "# There are a lot of family ids, so we'll compress all of the families under 3 members into one code.\n",
    "family_ids[titanic[\"FamilySize\"] < 3] = -1\n",
    "\n",
    "# Print the count of each unique id.\n",
    "print(pandas.value_counts(family_ids))\n",
    "\n",
    "titanic[\"FamilyId\"] = family_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, more features doesn't mean more accurate prediction, in fact, we should only select features that correlate more with the survival data.\n",
    "\n",
    "We can use sklearn's SelectKBest function, that helps us select the best features from the data, and allows us to select how many features to to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAEsCAYAAAAIBeLrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f91eb736e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.811447811448\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform from p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "# Plot the scores.  See how \"Pclass\", \"Sex\", \"Title\", and \"Fare\" are the best?\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "# Pick only the four best features.\n",
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Title\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=8, min_samples_leaf=4)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic['Survived'], cv=3)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the feature that correlates to survivability is Sex, Passenger Class, Title and Fare.\n",
    "\n",
    "Aside from feature selection, we can also use Gradient Boosting on our classifier, which involves training decision trees one after another to recognize the errors of the trees before it.\n",
    "\n",
    "And of course, we can use different classifiers to generate predictions which we can combine to have a more realistic approximation. While similar classifiers like a random forests and a decision tree might not work well because of their similarity, combining it with a linear regression works well.\n",
    "\n",
    "In the next example, we're using a gradient boosting classifier and linear regression to estimate prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.819304152637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rogue/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:37: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import numpy as np\n",
    "\n",
    "# The algorithms we want to ensemble.\n",
    "# We're using the more linear predictors for the logistic regression, and everything with the gradient boosting classifier.\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "# Initialize the cross validation folds\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    full_test_predictions = []\n",
    "    # Make predictions for each algorithm on each fold\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm on the training data.\n",
    "        alg.fit(titanic[predictors].iloc[train,:], train_target)\n",
    "        # Select and predict on the test fold.  \n",
    "        # The .astype(float) is necessary to convert the dataframe to all floats and avoid an sklearn error.\n",
    "        test_predictions = alg.predict_proba(titanic[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_predictions.append(test_predictions)\n",
    "    # Use a simple ensembling scheme -- just average the predictions to get the final classification.\n",
    "    test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
    "    # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction.\n",
    "    test_predictions[test_predictions <= .5] = 0\n",
    "    test_predictions[test_predictions > .5] = 1\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "# Put all the predictions together into one array.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Compute accuracy by comparing to the training data.\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've familiarized ourselves on ways to improve our prediction, we may start on using our algorithm on our test data.\n",
    "\n",
    "Like in the first step, let's prepare our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived  Pclass  \\\n",
      "0              1         0       3   \n",
      "1              2         1       1   \n",
      "2              3         1       3   \n",
      "3              4         1       1   \n",
      "4              5         0       3   \n",
      "5              6         0       3   \n",
      "6              7         0       1   \n",
      "7              8         0       3   \n",
      "8              9         1       3   \n",
      "9             10         1       2   \n",
      "10            11         1       3   \n",
      "11            12         1       1   \n",
      "12            13         0       3   \n",
      "13            14         0       3   \n",
      "14            15         0       3   \n",
      "15            16         1       2   \n",
      "16            17         0       3   \n",
      "17            18         1       2   \n",
      "18            19         0       3   \n",
      "19            20         1       3   \n",
      "20            21         0       2   \n",
      "21            22         1       2   \n",
      "22            23         1       3   \n",
      "23            24         1       1   \n",
      "24            25         0       3   \n",
      "25            26         1       3   \n",
      "26            27         0       3   \n",
      "27            28         0       1   \n",
      "28            29         1       3   \n",
      "29            30         0       3   \n",
      "..           ...       ...     ...   \n",
      "861          862         0       2   \n",
      "862          863         1       1   \n",
      "863          864         0       3   \n",
      "864          865         0       2   \n",
      "865          866         1       2   \n",
      "866          867         1       2   \n",
      "867          868         0       1   \n",
      "868          869         0       3   \n",
      "869          870         1       3   \n",
      "870          871         0       3   \n",
      "871          872         1       1   \n",
      "872          873         0       1   \n",
      "873          874         0       3   \n",
      "874          875         1       2   \n",
      "875          876         1       3   \n",
      "876          877         0       3   \n",
      "877          878         0       3   \n",
      "878          879         0       3   \n",
      "879          880         1       1   \n",
      "880          881         1       2   \n",
      "881          882         0       3   \n",
      "882          883         0       3   \n",
      "883          884         0       2   \n",
      "884          885         0       3   \n",
      "885          886         0       3   \n",
      "886          887         0       2   \n",
      "887          888         1       1   \n",
      "888          889         0       3   \n",
      "889          890         1       1   \n",
      "890          891         0       3   \n",
      "\n",
      "                                                  Name Sex   Age  SibSp  \\\n",
      "0                              Braund, Mr. Owen Harris   0  22.0      1   \n",
      "1    Cumings, Mrs. John Bradley (Florence Briggs Th...   1  38.0      1   \n",
      "2                               Heikkinen, Miss. Laina   1  26.0      0   \n",
      "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)   1  35.0      1   \n",
      "4                             Allen, Mr. William Henry   0  35.0      0   \n",
      "5                                     Moran, Mr. James   0  28.0      0   \n",
      "6                              McCarthy, Mr. Timothy J   0  54.0      0   \n",
      "7                       Palsson, Master. Gosta Leonard   0   2.0      3   \n",
      "8    Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)   1  27.0      0   \n",
      "9                  Nasser, Mrs. Nicholas (Adele Achem)   1  14.0      1   \n",
      "10                     Sandstrom, Miss. Marguerite Rut   1   4.0      1   \n",
      "11                            Bonnell, Miss. Elizabeth   1  58.0      0   \n",
      "12                      Saundercock, Mr. William Henry   0  20.0      0   \n",
      "13                         Andersson, Mr. Anders Johan   0  39.0      1   \n",
      "14                Vestrom, Miss. Hulda Amanda Adolfina   1  14.0      0   \n",
      "15                    Hewlett, Mrs. (Mary D Kingcome)    1  55.0      0   \n",
      "16                                Rice, Master. Eugene   0   2.0      4   \n",
      "17                        Williams, Mr. Charles Eugene   0  28.0      0   \n",
      "18   Vander Planke, Mrs. Julius (Emelia Maria Vande...   1  31.0      1   \n",
      "19                             Masselmani, Mrs. Fatima   1  28.0      0   \n",
      "20                                Fynney, Mr. Joseph J   0  35.0      0   \n",
      "21                               Beesley, Mr. Lawrence   0  34.0      0   \n",
      "22                         McGowan, Miss. Anna \"Annie\"   1  15.0      0   \n",
      "23                        Sloper, Mr. William Thompson   0  28.0      0   \n",
      "24                       Palsson, Miss. Torborg Danira   1   8.0      3   \n",
      "25   Asplund, Mrs. Carl Oscar (Selma Augusta Emilia...   1  38.0      1   \n",
      "26                             Emir, Mr. Farred Chehab   0  28.0      0   \n",
      "27                      Fortune, Mr. Charles Alexander   0  19.0      3   \n",
      "28                       O'Dwyer, Miss. Ellen \"Nellie\"   1  28.0      0   \n",
      "29                                 Todoroff, Mr. Lalio   0  28.0      0   \n",
      "..                                                 ...  ..   ...    ...   \n",
      "861                        Giles, Mr. Frederick Edward   0  21.0      1   \n",
      "862  Swift, Mrs. Frederick Joel (Margaret Welles Ba...   1  48.0      0   \n",
      "863                  Sage, Miss. Dorothy Edith \"Dolly\"   1  28.0      8   \n",
      "864                             Gill, Mr. John William   0  24.0      0   \n",
      "865                           Bystrom, Mrs. (Karolina)   1  42.0      0   \n",
      "866                       Duran y More, Miss. Asuncion   1  27.0      1   \n",
      "867               Roebling, Mr. Washington Augustus II   0  31.0      0   \n",
      "868                        van Melkebeke, Mr. Philemon   0  28.0      0   \n",
      "869                    Johnson, Master. Harold Theodor   0   4.0      1   \n",
      "870                                  Balkic, Mr. Cerin   0  26.0      0   \n",
      "871   Beckwith, Mrs. Richard Leonard (Sallie Monypeny)   1  47.0      1   \n",
      "872                           Carlsson, Mr. Frans Olof   0  33.0      0   \n",
      "873                        Vander Cruyssen, Mr. Victor   0  47.0      0   \n",
      "874              Abelson, Mrs. Samuel (Hannah Wizosky)   1  28.0      1   \n",
      "875                   Najib, Miss. Adele Kiamie \"Jane\"   1  15.0      0   \n",
      "876                      Gustafsson, Mr. Alfred Ossian   0  20.0      0   \n",
      "877                               Petroff, Mr. Nedelio   0  19.0      0   \n",
      "878                                 Laleff, Mr. Kristo   0  28.0      0   \n",
      "879      Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)   1  56.0      0   \n",
      "880       Shelley, Mrs. William (Imanita Parrish Hall)   1  25.0      0   \n",
      "881                                 Markun, Mr. Johann   0  33.0      0   \n",
      "882                       Dahlberg, Miss. Gerda Ulrika   1  22.0      0   \n",
      "883                      Banfield, Mr. Frederick James   0  28.0      0   \n",
      "884                             Sutehall, Mr. Henry Jr   0  25.0      0   \n",
      "885               Rice, Mrs. William (Margaret Norton)   1  39.0      0   \n",
      "886                              Montvila, Rev. Juozas   0  27.0      0   \n",
      "887                       Graham, Miss. Margaret Edith   1  19.0      0   \n",
      "888           Johnston, Miss. Catherine Helen \"Carrie\"   1  28.0      1   \n",
      "889                              Behr, Mr. Karl Howell   0  26.0      0   \n",
      "890                                Dooley, Mr. Patrick   0  32.0      0   \n",
      "\n",
      "     Parch            Ticket      Fare        Cabin Embarked  FamilySize  \\\n",
      "0        0         A/5 21171    7.2500          NaN        0           1   \n",
      "1        0          PC 17599   71.2833          C85        1           1   \n",
      "2        0  STON/O2. 3101282    7.9250          NaN        0           0   \n",
      "3        0            113803   53.1000         C123        0           1   \n",
      "4        0            373450    8.0500          NaN        0           0   \n",
      "5        0            330877    8.4583          NaN        2           0   \n",
      "6        0             17463   51.8625          E46        0           0   \n",
      "7        1            349909   21.0750          NaN        0           4   \n",
      "8        2            347742   11.1333          NaN        0           2   \n",
      "9        0            237736   30.0708          NaN        1           1   \n",
      "10       1           PP 9549   16.7000           G6        0           2   \n",
      "11       0            113783   26.5500         C103        0           0   \n",
      "12       0         A/5. 2151    8.0500          NaN        0           0   \n",
      "13       5            347082   31.2750          NaN        0           6   \n",
      "14       0            350406    7.8542          NaN        0           0   \n",
      "15       0            248706   16.0000          NaN        0           0   \n",
      "16       1            382652   29.1250          NaN        2           5   \n",
      "17       0            244373   13.0000          NaN        0           0   \n",
      "18       0            345763   18.0000          NaN        0           1   \n",
      "19       0              2649    7.2250          NaN        1           0   \n",
      "20       0            239865   26.0000          NaN        0           0   \n",
      "21       0            248698   13.0000          D56        0           0   \n",
      "22       0            330923    8.0292          NaN        2           0   \n",
      "23       0            113788   35.5000           A6        0           0   \n",
      "24       1            349909   21.0750          NaN        0           4   \n",
      "25       5            347077   31.3875          NaN        0           6   \n",
      "26       0              2631    7.2250          NaN        1           0   \n",
      "27       2             19950  263.0000  C23 C25 C27        0           5   \n",
      "28       0            330959    7.8792          NaN        2           0   \n",
      "29       0            349216    7.8958          NaN        0           0   \n",
      "..     ...               ...       ...          ...      ...         ...   \n",
      "861      0             28134   11.5000          NaN        0           1   \n",
      "862      0             17466   25.9292          D17        0           0   \n",
      "863      2          CA. 2343   69.5500          NaN        0          10   \n",
      "864      0            233866   13.0000          NaN        0           0   \n",
      "865      0            236852   13.0000          NaN        0           0   \n",
      "866      0     SC/PARIS 2149   13.8583          NaN        1           1   \n",
      "867      0          PC 17590   50.4958          A24        0           0   \n",
      "868      0            345777    9.5000          NaN        0           0   \n",
      "869      1            347742   11.1333          NaN        0           2   \n",
      "870      0            349248    7.8958          NaN        0           0   \n",
      "871      1             11751   52.5542          D35        0           2   \n",
      "872      0               695    5.0000  B51 B53 B55        0           0   \n",
      "873      0            345765    9.0000          NaN        0           0   \n",
      "874      0         P/PP 3381   24.0000          NaN        1           1   \n",
      "875      0              2667    7.2250          NaN        1           0   \n",
      "876      0              7534    9.8458          NaN        0           0   \n",
      "877      0            349212    7.8958          NaN        0           0   \n",
      "878      0            349217    7.8958          NaN        0           0   \n",
      "879      1             11767   83.1583          C50        1           1   \n",
      "880      1            230433   26.0000          NaN        0           1   \n",
      "881      0            349257    7.8958          NaN        0           0   \n",
      "882      0              7552   10.5167          NaN        0           0   \n",
      "883      0  C.A./SOTON 34068   10.5000          NaN        0           0   \n",
      "884      0   SOTON/OQ 392076    7.0500          NaN        0           0   \n",
      "885      5            382652   29.1250          NaN        2           5   \n",
      "886      0            211536   13.0000          NaN        0           0   \n",
      "887      0            112053   30.0000          B42        0           0   \n",
      "888      2        W./C. 6607   23.4500          NaN        0           3   \n",
      "889      0            111369   30.0000         C148        1           0   \n",
      "890      0            370376    7.7500          NaN        2           0   \n",
      "\n",
      "     NameLength Title  FamilyId  \n",
      "0            23     1        -1  \n",
      "1            51     3        -1  \n",
      "2            22     2        -1  \n",
      "3            44     3        -1  \n",
      "4            24     1        -1  \n",
      "5            16     1        -1  \n",
      "6            23     1        -1  \n",
      "7            30     4         8  \n",
      "8            49     3        -1  \n",
      "9            35     3        -1  \n",
      "10           31     2        -1  \n",
      "11           24     2        -1  \n",
      "12           30     1        -1  \n",
      "13           27     1        14  \n",
      "14           36     2        -1  \n",
      "15           32     3        -1  \n",
      "16           20     4        17  \n",
      "17           28     1        -1  \n",
      "18           55     3        -1  \n",
      "19           23     3        -1  \n",
      "20           20     1        -1  \n",
      "21           21     1        -1  \n",
      "22           27     2        -1  \n",
      "23           28     1        -1  \n",
      "24           29     2         8  \n",
      "25           57     3        25  \n",
      "26           23     1        -1  \n",
      "27           30     1        27  \n",
      "28           29     2        -1  \n",
      "29           19     1        -1  \n",
      "..          ...   ...       ...  \n",
      "861          27     1        -1  \n",
      "862          51     3        -1  \n",
      "863          33     2       149  \n",
      "864          22     1        -1  \n",
      "865          24     3        -1  \n",
      "866          28     2        -1  \n",
      "867          36     1        -1  \n",
      "868          27     1        -1  \n",
      "869          31     4        -1  \n",
      "870          17     1        -1  \n",
      "871          48     3        -1  \n",
      "872          24     1        -1  \n",
      "873          27     1        -1  \n",
      "874          37     3        -1  \n",
      "875          32     2        -1  \n",
      "876          29     1        -1  \n",
      "877          20     1        -1  \n",
      "878          18     1        -1  \n",
      "879          45     3        -1  \n",
      "880          44     3        -1  \n",
      "881          18     1        -1  \n",
      "882          28     2        -1  \n",
      "883          29     1        -1  \n",
      "884          22     1        -1  \n",
      "885          36     3        17  \n",
      "886          21     6        -1  \n",
      "887          28     2        -1  \n",
      "888          40     2       633  \n",
      "889          21     1        -1  \n",
      "890          19     1        -1  \n",
      "\n",
      "[891 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "# LOAD TRAINING AND TEST DATA  --------------------------------\n",
    "\n",
    "training_df = pd.read_csv(\"data/train.csv\")\n",
    "testing_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# CLEAN UP / FIX DATA  --------------------------------\n",
    "\n",
    "training_df[\"Age\"] = training_df[\"Age\"].fillna(training_df[\"Age\"].median())\n",
    "training_df[\"Fare\"] = training_df[\"Fare\"].fillna(training_df[\"Fare\"].median())\n",
    "training_df[\"Embarked\"] = training_df[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "training_df.loc[training_df[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "training_df.loc[training_df[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "training_df.loc[training_df[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "training_df.loc[training_df[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "training_df.loc[training_df[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n",
    "\n",
    "testing_df[\"Age\"] = testing_df[\"Age\"].fillna(testing_df[\"Age\"].median())\n",
    "testing_df[\"Fare\"] = testing_df[\"Fare\"].fillna(testing_df[\"Fare\"].median())\n",
    "testing_df[\"Embarked\"] = testing_df[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "testing_df.loc[testing_df[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "testing_df.loc[testing_df[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "testing_df.loc[testing_df[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "testing_df.loc[testing_df[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "testing_df.loc[testing_df[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n",
    "\n",
    "# FEATURE FUNCTIONS / VARIABLES  --------------------------------\n",
    "\n",
    "training_df[\"FamilySize\"] = training_df[\"SibSp\"] + training_df[\"Parch\"]\n",
    "training_df[\"NameLength\"] = training_df[\"Name\"].apply(lambda x: len(x))\n",
    "\n",
    "testing_df[\"FamilySize\"] = testing_df[\"SibSp\"] + testing_df[\"Parch\"]\n",
    "testing_df[\"NameLength\"] = testing_df[\"Name\"].apply(lambda x: len(x))\n",
    "\n",
    "title_mapping = {\n",
    "    \"Mr\": 1,\n",
    "    \"Miss\": 2,\n",
    "    \"Mrs\": 3,\n",
    "    \"Master\": 4,\n",
    "    \"Dr\": 5,\n",
    "    \"Rev\": 6,\n",
    "    \"Major\": 7,\n",
    "    \"Col\": 7,\n",
    "    \"Mlle\": 8,\n",
    "    \"Mme\": 8,\n",
    "    \"Don\": 9,\n",
    "    \"Lady\": 10,\n",
    "    \"Countess\": 10,\n",
    "    \"Jonkheer\": 10,\n",
    "    \"Sir\": 9,\n",
    "    \"Capt\": 7,\n",
    "    \"Ms\": 2,\n",
    "    \"Dona\": 10\n",
    "}\n",
    "\n",
    "# extract title from passengers name\n",
    "def get_title(name):\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "family_id_mapping = {}\n",
    "\n",
    "# extract last name and assign / add family id \n",
    "def get_family_id(row):\n",
    "    # find last name \n",
    "    last_name = row[\"Name\"].split(\",\")[0]\n",
    "    # create family id\n",
    "    family_id = \"{0}{1}\".format(last_name, row[\"FamilySize\"])\n",
    "    # look up the id in the mapping\n",
    "    if family_id not in family_id_mapping:\n",
    "        if len(family_id_mapping) == 0:\n",
    "            current_id = 1\n",
    "        else:\n",
    "            # Get the max id from the mapping and add one if\n",
    "            # we don't have an id\n",
    "            current_id = (max(family_id_mapping.items(), \n",
    "                key=operator.itemgetter(1))[1] + 1)\n",
    "        family_id_mapping[family_id] = current_id\n",
    "    return family_id_mapping[family_id]\n",
    "\n",
    "\n",
    "# APPLY FEATURES   --------------------------------\n",
    "\n",
    "training_titles = training_df[\"Name\"].apply(get_title)\n",
    "testing_titles = testing_df[\"Name\"].apply(get_title)\n",
    "\n",
    "for k, v in title_mapping.items():\n",
    "    training_titles[training_titles == k] = v\n",
    "    testing_titles[testing_titles == k] = v\n",
    "\n",
    "training_df[\"Title\"] = training_titles\n",
    "testing_df[\"Title\"] = testing_titles\n",
    "\n",
    "\n",
    "# Get the family ids with the apply method\n",
    "family_ids = training_df.apply(get_family_id, axis=1)\n",
    "test_family_ids = testing_df.apply(get_family_id, axis=1)\n",
    "\n",
    "# Categorize familysize < 3 into one\n",
    "family_ids[training_df[\"FamilySize\"] < 3] = -1\n",
    "test_family_ids[testing_df[\"FamilySize\"] < 3] = -1\n",
    "\n",
    "training_df[\"FamilyId\"] = family_ids\n",
    "testing_df[\"FamilyId\"] = test_family_ids\n",
    "print training_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then apply our algorithm to the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.851851851852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rogue/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:30: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors],\n",
    "    [LogisticRegression(random_state=1), predictors]\n",
    "]\n",
    "\n",
    "full_predictions = []\n",
    "td_predictions = []\n",
    "for alg, predictors in algorithms:\n",
    "    # Fit the algorithm using the full training data.\n",
    "    alg.fit(training_df[predictors], training_df[\"Survived\"])\n",
    "    # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error.\n",
    "    predictions = alg.predict_proba(testing_df[predictors].astype(float))[:,1]\n",
    "    training_data_prediction = alg.predict_proba(training_df[predictors].astype(float))[:,1]\n",
    "    full_predictions.append(predictions)\n",
    "    td_predictions.append(training_data_prediction)\n",
    "\n",
    "# The gradient boosting classifier generates better predictions, so we weight it higher.\n",
    "full_predictions = (full_predictions[full_predictions==0] * 3 + full_predictions[full_predictions==1]) / 4\n",
    "full_predictions[full_predictions <= .5] = 0\n",
    "full_predictions[full_predictions > .5] = 1\n",
    "\n",
    "td_predictions = (td_predictions[td_predictions==0] * 3 + td_predictions[td_predictions==1]) / 4\n",
    "td_predictions[td_predictions <= .5] = 0\n",
    "td_predictions[td_predictions > .5] = 1\n",
    "\n",
    "full_predictions = full_predictions.astype(int)\n",
    "\n",
    "accuracy = sum(td_predictions[td_predictions == training_df[\"Survived\"]]) / len(td_predictions)\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And that's it!\n",
    "We can add the code block to generate a submission file to Kaggle. Although we've got around 0.85% accuracy on the training data. The test data is likely to have a different result. This is perhaps due to some columns or relationships that are more apparent in the test data. \n",
    "Well, good luck and happy coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\": testing_df[\"PassengerId\"],\n",
    "    \"Survived\": predictions\n",
    "   })\n",
    "\n",
    "submission.to_csv(\"data/kaggle-titanic-2016-07-18.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
